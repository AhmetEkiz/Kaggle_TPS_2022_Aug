{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ahmetekiz/tps-aug-2022-blending?scriptVersionId=105980532\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# Tabular Playground Series - Aug 2022\n\nIn this notebook, I create base model for TPS Aug 2022. In below, some information about data is gave us. \n\n>This data represents the results of a large product testing study. For each **product_code** you are given a number of product **attributes** (fixed for the code) as well as a number of **measurement values** for each individual product, representing various lab testing methods. Each product is used in a simulated real-world environment experiment, and and absorbs a certain amount of fluid (**loading**) to see whether or not it fails.\n\n>Your task is to use the data to predict individual product failures of new codes with their individual lab test results.\n\n**Evaluation**: Submissions are evaluated on area under the ROC curve between the **predicted probability** and the observed target.\n\n1. My First Notebook on this Competition: https://www.kaggle.com/code/ahmetekiz/tps-aug-2022-starter\n\n### This notebook's progress\nBlending auc score - submission score\n1. optimized models blending and with missing values columns: 0.58764 - 0.58318\n1. basic models blending and without missing values columns: 0.58799 - 0.58473","metadata":{}},{"cell_type":"markdown","source":"<a id=\"0\"></a> <br>\n# Table of Contents    \n1. [A Glance at the Data](#2) \n1. [Missing Values](#4)\n1. [Preprocess](#5)\n1. [Create Folds](#6)\n1. [Train and Make Predictions](#7)\n    1. [LogisticRegression](#8)\n    1. [XGBoost](#9)\n    1. [CatBoostClassifier](#10)\n    1. [LGBMClassifier](#11)\n    1. [Tensorflow ANN Model](#14)\n1. [Merge New Sets](#17)\n1. [Blending Results](#18)\n1. [Submission](#30)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n# A Glance at the Data","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n# Preprocess\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn import model_selection\n\n# Metrics\nfrom sklearn.metrics import roc_auc_score\n\n# Model\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.svm import SVC\n\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom colorama import Fore, Back, Style\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-31T22:56:30.314271Z","iopub.execute_input":"2022-08-31T22:56:30.314973Z","iopub.status.idle":"2022-08-31T22:56:31.895854Z","shell.execute_reply.started":"2022-08-31T22:56:30.314889Z","shell.execute_reply":"2022-08-31T22:56:31.894873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_full = pd.read_csv(\"/kaggle/input/tabular-playground-series-aug-2022/train.csv\", index_col='id')\ndf_test = pd.read_csv(\"/kaggle/input/tabular-playground-series-aug-2022/test.csv\", index_col='id')\n\ndf_train = df_train_full.drop(['failure'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T22:56:31.897612Z","iopub.execute_input":"2022-08-31T22:56:31.898046Z","iopub.status.idle":"2022-08-31T22:56:32.054089Z","shell.execute_reply.started":"2022-08-31T22:56:31.898003Z","shell.execute_reply":"2022-08-31T22:56:32.053114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_full.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-31T22:56:32.055671Z","iopub.execute_input":"2022-08-31T22:56:32.056024Z","iopub.status.idle":"2022-08-31T22:56:32.085378Z","shell.execute_reply.started":"2022-08-31T22:56:32.055988Z","shell.execute_reply":"2022-08-31T22:56:32.084176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_full.info()","metadata":{"execution":{"iopub.status.busy":"2022-08-31T22:56:32.089635Z","iopub.execute_input":"2022-08-31T22:56:32.090099Z","iopub.status.idle":"2022-08-31T22:56:32.10973Z","shell.execute_reply.started":"2022-08-31T22:56:32.090049Z","shell.execute_reply":"2022-08-31T22:56:32.108665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_full.describe()","metadata":{"execution":{"iopub.status.busy":"2022-08-31T22:56:32.111086Z","iopub.execute_input":"2022-08-31T22:56:32.111653Z","iopub.status.idle":"2022-08-31T22:56:32.204306Z","shell.execute_reply.started":"2022-08-31T22:56:32.111617Z","shell.execute_reply":"2022-08-31T22:56:32.203201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ratio of the Failure on Train Set","metadata":{}},{"cell_type":"code","source":"colors = [\"#22a7f0\",\"#c9e52f\"]\nlabels = ['Normal', 'Failure']\nexplode = [0,0.05]\n\nprint(df_train_full.failure.value_counts())\n           \nplt.figure(figsize = (5,5))\nplt.pie(df_train_full.failure.value_counts(),explode=explode, labels=labels, colors=colors, autopct='%1.1f%%')\nplt.title(\"Failure Rate of Products on Training Set\", color = 'blue',fontsize = 14)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T22:56:32.206127Z","iopub.execute_input":"2022-08-31T22:56:32.206557Z","iopub.status.idle":"2022-08-31T22:56:32.320388Z","shell.execute_reply.started":"2022-08-31T22:56:32.206521Z","shell.execute_reply":"2022-08-31T22:56:32.319399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ratio of Nan Values\nThere is a lot of missing value. We will deal with them later.","metadata":{}},{"cell_type":"code","source":"cm = sns.light_palette(\"yellow\", as_cmap=True)\npd.DataFrame({\"NaN Count\": df_train_full.isna().sum(),\n              \"NaN Ratio\": df_train_full.isna().sum()/len(df_train_full)}).sort_values(by=\"NaN Count\",\n                                                                 ascending=False).style.background_gradient(cmap=cm)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T22:56:32.322162Z","iopub.execute_input":"2022-08-31T22:56:32.322867Z","iopub.status.idle":"2022-08-31T22:56:32.445709Z","shell.execute_reply.started":"2022-08-31T22:56:32.32283Z","shell.execute_reply":"2022-08-31T22:56:32.44417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a> <br>\n# Missing Values","metadata":{}},{"cell_type":"code","source":"df_train.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-08-31T22:56:32.447303Z","iopub.execute_input":"2022-08-31T22:56:32.447988Z","iopub.status.idle":"2022-08-31T22:56:32.461562Z","shell.execute_reply.started":"2022-08-31T22:56:32.447949Z","shell.execute_reply":"2022-08-31T22:56:32.460464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-warning\">\n<b>Remember:</b> Null Values can be a sign of failure.\n</div>","metadata":{}},{"cell_type":"code","source":"cols_with_missing = [col for col in df_train.columns if df_train[col].isnull().any()]\ncols_with_missing","metadata":{"execution":{"iopub.status.busy":"2022-08-31T22:56:32.463277Z","iopub.execute_input":"2022-08-31T22:56:32.463936Z","iopub.status.idle":"2022-08-31T22:56:32.481177Z","shell.execute_reply.started":"2022-08-31T22:56:32.463895Z","shell.execute_reply":"2022-08-31T22:56:32.480192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# new columns for missing columns\nnew_columns = []\nfor col in cols_with_missing:\n    new_columns.append(f\"{col}_was_missing\")\n\nprint(new_columns)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T22:56:32.482567Z","iopub.execute_input":"2022-08-31T22:56:32.483629Z","iopub.status.idle":"2022-08-31T22:56:32.489542Z","shell.execute_reply.started":"2022-08-31T22:56:32.48359Z","shell.execute_reply":"2022-08-31T22:56:32.488531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical_cols = [c for c in df_train.columns if df_train[c].dtypes in ['int', 'float']]\nprint(\"Numerical Columns\\n\", numerical_cols)\n\ncategorical_cols = [c for c in df_train.columns if df_train[c].dtypes in ['object']]\nprint(\"\\nCategorical Columns\\n\", categorical_cols)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T22:56:32.490977Z","iopub.execute_input":"2022-08-31T22:56:32.492163Z","iopub.status.idle":"2022-08-31T22:56:32.501005Z","shell.execute_reply.started":"2022-08-31T22:56:32.492101Z","shell.execute_reply":"2022-08-31T22:56:32.499788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# numerical_cols += new_columns\n# print(numerical_cols)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T22:56:32.503039Z","iopub.execute_input":"2022-08-31T22:56:32.503589Z","iopub.status.idle":"2022-08-31T22:56:32.508427Z","shell.execute_reply.started":"2022-08-31T22:56:32.503551Z","shell.execute_reply":"2022-08-31T22:56:32.507387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5\"></a> <br>\n# Preprocess","metadata":{}},{"cell_type":"code","source":"# Preprocessing for numerical data\n# it was constant\nnumerical_transformer = Pipeline(steps=[('imputer',SimpleImputer(strategy='median')),\n                                        ('std_scaler', StandardScaler())\n                                       ]) \n\n\n# # Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer([\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n])","metadata":{"execution":{"iopub.status.busy":"2022-08-31T22:56:32.514404Z","iopub.execute_input":"2022-08-31T22:56:32.51541Z","iopub.status.idle":"2022-08-31T22:56:32.522088Z","shell.execute_reply.started":"2022-08-31T22:56:32.515372Z","shell.execute_reply":"2022-08-31T22:56:32.521028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Apply Preprocess\nWhen I use preprocess on kfold, preprocess can't deal with test data columns categories that training data doesn't contain. So, I make preprocess before everything. ??? ","metadata":{}},{"cell_type":"code","source":"def do_preprocess(df, preprocessor, train = False, shapes = False, missing_columns=False):\n    if missing_columns:\n        for col in cols_with_missing:\n            df[col + '_was_missing'] = df[col].isnull()\n    \n    if train==True:\n        df = preprocessor.fit_transform(df)\n    else:\n        df = preprocessor.transform(df)\n        \n    if shapes:\n        print(df.shape)\n    else:\n        pass\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-08-31T22:56:32.5238Z","iopub.execute_input":"2022-08-31T22:56:32.524307Z","iopub.status.idle":"2022-08-31T22:56:32.532088Z","shell.execute_reply.started":"2022-08-31T22:56:32.524244Z","shell.execute_reply":"2022-08-31T22:56:32.530992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = do_preprocess(df_train, preprocessor, train=True, shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T22:56:32.533987Z","iopub.execute_input":"2022-08-31T22:56:32.534944Z","iopub.status.idle":"2022-08-31T22:56:32.664101Z","shell.execute_reply.started":"2022-08-31T22:56:32.534915Z","shell.execute_reply":"2022-08-31T22:56:32.662964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## New Columns After Preprocessing","metadata":{}},{"cell_type":"code","source":"for c in preprocessor.named_transformers_['cat']['onehot'].categories_:\n    print(c)\n\ncat_one_hot_attribs = np.concatenate([c for c in preprocessor.named_transformers_['cat']['onehot'].categories_])\nprint(\"\\nCategorical Columns:\\n\", cat_one_hot_attribs) \nprint(\"\\nNumerical Columns:\\n\", numerical_cols)\n\nall_cols = np.concatenate((numerical_cols, cat_one_hot_attribs))\nprint(\"\\nAll Columns:\\n\", all_cols)\nprint(\"\\nAll Columns Shape:\", all_cols.shape)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T22:56:32.666018Z","iopub.execute_input":"2022-08-31T22:56:32.666444Z","iopub.status.idle":"2022-08-31T22:56:32.674433Z","shell.execute_reply.started":"2022-08-31T22:56:32.666405Z","shell.execute_reply":"2022-08-31T22:56:32.673147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show feature importances\n# code source : https://www.kaggle.com/code/ambrosm/tpsaug22-eda-which-makes-sense\ndef plot_model_feature_importance(feature_importance_list, features, title_name='Model Feature Importance', number_of_features=10):\n    \"\"\"\n    features_importance_list: array of features importance\n    features: a array that contain columns or features names\n    title_name = title of graph\n    number_of_features : how many numbers of features to show on the graph\n    \"\"\"\n    importance_df = pd.DataFrame(np.array(feature_importance_list).T, index=features)\n    importance_df['mean'] = importance_df.mean(axis=1).abs()\n    importance_df['feature'] = features\n    importance_df = importance_df.sort_values('mean', ascending=False).reset_index().head(number_of_features)\n    plt.figure(figsize=(14, 4))\n    plt.barh(importance_df.index, importance_df['mean'], color='lightgreen')\n    plt.gca().invert_yaxis()\n    plt.yticks(ticks=importance_df.index, labels=importance_df['feature'])\n    plt.title(title_name)\n    plt.show()\n    \n    return importance_df # to show the dataframe\n\n# importance_df = plot_model_feature_importance(feature_importances, all_cols, number_of_features=20)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T22:56:32.675979Z","iopub.execute_input":"2022-08-31T22:56:32.677034Z","iopub.status.idle":"2022-08-31T22:56:32.686646Z","shell.execute_reply.started":"2022-08-31T22:56:32.676996Z","shell.execute_reply":"2022-08-31T22:56:32.68556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6\"></a> <br>\n# Create Folds","metadata":{}},{"cell_type":"code","source":"# df_train_full is for kfold\n\ndf_train_full = df_train_full.copy()\ndf_train_full[\"kfold\"] = -1 \n\ndf_train_full.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-31T22:56:32.688971Z","iopub.execute_input":"2022-08-31T22:56:32.690069Z","iopub.status.idle":"2022-08-31T22:56:32.722048Z","shell.execute_reply.started":"2022-08-31T22:56:32.690031Z","shell.execute_reply":"2022-08-31T22:56:32.720851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold, (train_indicies, valid_indicies) in enumerate(kf.split(X=df_train_full)):\n    df_train_full.loc[valid_indicies, \"kfold\"] = fold\n\ndf_train_full.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-31T22:56:32.723604Z","iopub.execute_input":"2022-08-31T22:56:32.724471Z","iopub.status.idle":"2022-08-31T22:56:32.761371Z","shell.execute_reply.started":"2022-08-31T22:56:32.724431Z","shell.execute_reply":"2022-08-31T22:56:32.760272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to save id's\ndf_train_full['id'] = df_train_full.index \ndf_train_full = df_train_full.reset_index(drop=True)\ndf_train_full.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-31T22:56:32.764631Z","iopub.execute_input":"2022-08-31T22:56:32.765463Z","iopub.status.idle":"2022-08-31T22:56:32.797739Z","shell.execute_reply.started":"2022-08-31T22:56:32.765424Z","shell.execute_reply":"2022-08-31T22:56:32.796735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7\"></a> <br>\n# Train and Make Predictions\n\nI used optimized parameters from: https://www.kaggle.com/code/juhjoo/0-5902-tps-aug-lightgbm-xgboost-ann-ensemble","metadata":{}},{"cell_type":"markdown","source":"<a id=\"8\"></a> <br>\n# LogisticRegression","metadata":{}},{"cell_type":"code","source":"%%time\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []  # roc auc scores\n\nfor fold in range(5): #5\n    x_train = df_train_full[df_train_full.kfold != fold].reset_index(drop=True)\n    x_val = df_train_full[df_train_full.kfold == fold].reset_index(drop=True)\n    x_test = df_test.copy()\n    \n    valid_ids = x_val['id'].values.tolist()\n#     print(valid_ids)\n    \n    y_train = x_train.failure\n    y_val = x_val.failure\n    \n    # to drop target colum\n    x_train = x_train.drop(['id','failure'], axis=1)\n    x_val = x_val.drop(['id','failure'], axis=1)\n#     print(x_train.head())\n    \n    #preprocess\n    # Bundle preprocessing for numerical and categorical data\n    preprocessor = ColumnTransformer([\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n        ])\n    \n\n    x_train = do_preprocess(x_train, preprocessor, train=True)\n    x_val = do_preprocess(x_val, preprocessor, train=False)\n    x_test = do_preprocess(x_test, preprocessor, train=False)\n    \n    #select model\n    model = LogisticRegression(max_iter=500, C=0.0001, penalty='l2', solver='newton-cg')\n#     model = LogisticRegression()\n    \n    # train and predict\n    model.fit(x_train, y_train)\n\n    #Evaluating on Validation Set\n    y_val_pred = model.predict_proba(x_val)[:,1]\n    score = roc_auc_score(y_val, y_val_pred)\n    print(f\"auc = {score:.5f}\")\n    scores.append(score)\n    print(y_val_pred.shape)\n    print(type(y_val_pred))\n    \n    # predict test set\n    y_test_pred = model.predict_proba(x_test)[:,1]    \n        \n    # save test and validation predictions on a list and a dict\n    final_test_predictions.append(y_test_pred)\n    final_valid_predictions.update(dict(zip(valid_ids, y_val_pred)))\n    \n\nprint(np.mean(scores), np.std(scores))\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_1\"]\nfinal_valid_predictions.to_csv(\"train_pred_1.csv\", index=False)\n\nsubmission = pd.DataFrame({'id': df_test.index, 'pred_1': y_test_pred})\nsubmission['pred_1'] = np.mean(np.column_stack(final_test_predictions), axis=1)\nsubmission.to_csv('test_pred_1.csv', index=False)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2022-08-31T22:56:32.800096Z","iopub.execute_input":"2022-08-31T22:56:32.800947Z","iopub.status.idle":"2022-08-31T22:56:34.561296Z","shell.execute_reply.started":"2022-08-31T22:56:32.800908Z","shell.execute_reply":"2022-08-31T22:56:34.560195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_valid_predictions.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-31T22:56:34.563091Z","iopub.execute_input":"2022-08-31T22:56:34.563789Z","iopub.status.idle":"2022-08-31T22:56:34.574112Z","shell.execute_reply.started":"2022-08-31T22:56:34.56375Z","shell.execute_reply":"2022-08-31T22:56:34.573089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_valid_predictions.shape","metadata":{"execution":{"iopub.status.busy":"2022-08-31T22:56:34.575603Z","iopub.execute_input":"2022-08-31T22:56:34.577376Z","iopub.status.idle":"2022-08-31T22:56:34.583837Z","shell.execute_reply.started":"2022-08-31T22:56:34.577339Z","shell.execute_reply":"2022-08-31T22:56:34.582713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"9\"></a> <br>\n# XGBoost","metadata":{}},{"cell_type":"code","source":"%%time\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []  # roc auc scores\n\nfor fold in range(5): #5\n    x_train =  df_train_full[df_train_full.kfold != fold].reset_index(drop=True)\n    x_val = df_train_full[df_train_full.kfold == fold].reset_index(drop=True)\n    x_test = df_test.copy()\n    \n    valid_ids = x_val.id.values.tolist()\n#     print(valid_ids)\n    \n    y_train = x_train.failure\n    y_val = x_val.failure\n    \n    # to drop target colum\n    x_train = x_train.drop(['id','failure'], axis=1)\n    x_val = x_val.drop(['id','failure'], axis=1)\n#     print(x_train.head())\n    \n    #preprocess\n    preprocessor = ColumnTransformer([\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n        ])\n    \n    x_train = do_preprocess(x_train, preprocessor, train=True, missing_columns=True)\n    x_val = do_preprocess(x_val, preprocessor, train=False, missing_columns=True)\n    x_test = do_preprocess(x_test, preprocessor, train=False, missing_columns=True)\n   \n    # select model\n    Best_trial = {'use_label_encoder': False, \n                  'n_estimators': 35410, \n                  'learning_rate': 0.020400576769972683, \n                  'subsample': 0.87, \n                  'colsample_bytree': 0.47, \n                  'max_depth': 7, \n                  'gamma': 9.200000000000001, \n                  'booster': 'gbtree', \n                  'reg_lambda': 0.08160432917864537, \n                  'reg_alpha': 0.16867965528493895, \n                  'random_state': 42, \n                  'n_jobs': 4, \n                  'min_child_weight': 9.16763314844842,\n                  'eval_metric': 'auc',  # auc, rmse, mae\n                  'objective': 'binary:logistic',\n                 'tree_method' : 'gpu_hist',}\n    \n    \n    model = XGBClassifier(**Best_trial)\n    \n#     model = XGBClassifier()\n        \n    # train and predict\n    model.fit(x_train, y_train)\n\n    #Evaluating on Validation Set\n    y_val_pred = model.predict_proba(x_val)[:,1]\n    score = roc_auc_score(y_val, y_val_pred)\n    print(f\"auc = {score:.5f}\")\n    scores.append(score)\n    \n    # predict test set\n    y_test_pred = model.predict_proba(x_test)[:,1]    \n        \n    # save test and validation predictions on a list and a dict\n    final_test_predictions.append(y_test_pred)\n    final_valid_predictions.update(dict(zip(valid_ids, y_val_pred)))\n    \n\nprint(np.mean(scores), np.std(scores))\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_2\"]\nfinal_valid_predictions.to_csv(\"train_pred_2.csv\", index=False)\n\nsubmission = pd.DataFrame({'id': df_test.index, 'pred_2': y_test_pred})\nsubmission['pred_2'] = np.mean(np.column_stack(final_test_predictions), axis=1)\nsubmission.to_csv('test_pred_2.csv', index=False)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2022-08-31T23:09:05.569647Z","iopub.execute_input":"2022-08-31T23:09:05.57006Z","iopub.status.idle":"2022-08-31T23:09:33.341588Z","shell.execute_reply.started":"2022-08-31T23:09:05.570026Z","shell.execute_reply":"2022-08-31T23:09:33.340534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"10\"></a> <br>\n# CatBoostClassifier","metadata":{}},{"cell_type":"code","source":"%%time\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []  # roc auc scores\n\nfor fold in range(5): #5\n    x_train =  df_train_full[df_train_full.kfold != fold].reset_index(drop=True)\n    x_val = df_train_full[df_train_full.kfold == fold].reset_index(drop=True)\n    x_test = df_test.copy()\n    \n    valid_ids = x_val.id.values.tolist()\n#     print(valid_ids)\n    \n    y_train = x_train.failure\n    y_val = x_val.failure\n    \n    # to drop target colum\n    x_train = x_train.drop(['id','failure'], axis=1)\n    x_val = x_val.drop(['id','failure'], axis=1)\n#     print(x_train.head())\n    \n    #preprocess\n    preprocessor = ColumnTransformer([\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n        ])\n    \n    x_train = do_preprocess(x_train, preprocessor, train=True, missing_columns=True)\n    x_val = do_preprocess(x_val, preprocessor, train=False, missing_columns=True)\n    x_test = do_preprocess(x_test, preprocessor, train=False, missing_columns=True)\n    \n    # select model\n    Best_trial = {'learning_rate': 0.3772498790776553, \n                  'l2_leaf_reg': 37.93184269682747, \n                  'bagging_temperature': 1.471236889897418, \n                  'random_strength': 1.7032726681064423, \n                  'depth': 1, \n                  'min_data_in_leaf': 232,\n                  'verbose': 0}\n    \n    model = CatBoostClassifier(**Best_trial)\n#     model = CatBoostClassifier(iterations=10,verbose=5)\n    \n    # train and predict\n    model.fit(x_train, y_train)\n\n    #Evaluating on Validation Set\n    y_val_pred = model.predict_proba(x_val)[:,1]\n    score = roc_auc_score(y_val, y_val_pred)\n    print(f\"auc = {score:.5f}\")\n    scores.append(score)\n    \n    # predict test set\n    y_test_pred = model.predict_proba(x_test)[:,1]    \n        \n    # save test and validation predictions on a list and a dict\n    final_test_predictions.append(y_test_pred)\n    final_valid_predictions.update(dict(zip(valid_ids, y_val_pred)))\n    \n\nprint(np.mean(scores), np.std(scores))\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_3\"]\nfinal_valid_predictions.to_csv(\"train_pred_3.csv\", index=False)\n\nsubmission = pd.DataFrame({'id': df_test.index, 'pred_3': y_test_pred})\nsubmission['pred_3'] = np.mean(np.column_stack(final_test_predictions), axis=1)\nsubmission.to_csv('test_pred_3.csv', index=False)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2022-08-31T23:07:02.234753Z","iopub.execute_input":"2022-08-31T23:07:02.235317Z","iopub.status.idle":"2022-08-31T23:07:27.205044Z","shell.execute_reply.started":"2022-08-31T23:07:02.23527Z","shell.execute_reply":"2022-08-31T23:07:27.204043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"11\"></a> <br>\n# LGBMClassifier","metadata":{}},{"cell_type":"code","source":"%%time\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []  # roc auc scores\n\nfor fold in range(5): #5\n    x_train =  df_train_full[df_train_full.kfold != fold].reset_index(drop=True)\n    x_val = df_train_full[df_train_full.kfold == fold].reset_index(drop=True)\n    x_test = df_test.copy()\n    \n    valid_ids = x_val.id.values.tolist()\n#     print(valid_ids)\n    \n    y_train = x_train.failure\n    y_val = x_val.failure\n    \n    # to drop target colum\n    x_train = x_train.drop(['id','failure'], axis=1)\n    x_val = x_val.drop(['id','failure'], axis=1)\n#     print(x_train.head())\n    \n    #preprocess\n    preprocessor = ColumnTransformer([\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n        ])\n    \n    x_train = do_preprocess(x_train, preprocessor, train=True, missing_columns=True)\n    x_val = do_preprocess(x_val, preprocessor, train=False, missing_columns=True)\n    x_test = do_preprocess(x_test, preprocessor, train=False, missing_columns=True)\n    \n    # select model\n    Best_trial = {'n_estimators': 13688, \n                  'reg_alpha': 0.00029898817414380905, \n                  'reg_lambda': 0.022911278881140307, \n                  'colsample_bytree': 0.9, \n                  'num_leaves': 923, \n                  'feature_fraction': 0.4880516337950229, \n                  'bagging_fraction': 0.9230012153122733, \n                  'bagging_freq': 2, \n                  'min_child_samples': 95, \n                  'subsample': 0.61, \n                  'learning_rate': 0.05092641982004301, \n                  'max_depth': 1, \n                  'random_state': 42, \n                  'n_jobs': 4,\n                  'metrics' : ['binary_logloss','auc']  # auc, rmse, mae\n                  }\n    \n    model = LGBMClassifier(**Best_trial)\n#     model = LGBMClassifier()\n    \n    # train and predict\n    model.fit(x_train, y_train)\n\n    #Evaluating on Validation Set\n    y_val_pred = model.predict_proba(x_val)[:,1]\n    score = roc_auc_score(y_val, y_val_pred)\n    print(f\"auc = {score:.5f}\")\n    scores.append(score)\n    \n    # predict test set\n    y_test_pred = model.predict_proba(x_test)[:,1]    \n        \n    # save test and validation predictions on a list and a dict\n    final_test_predictions.append(y_test_pred)\n    final_valid_predictions.update(dict(zip(valid_ids, y_val_pred)))\n    \n\nprint(np.mean(scores), np.std(scores))\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_4\"]\nfinal_valid_predictions.to_csv(\"train_pred_4.csv\", index=False)\n\nsubmission = pd.DataFrame({'id': df_test.index, 'pred_4': y_test_pred})\nsubmission['pred_4'] = np.mean(np.column_stack(final_test_predictions), axis=1)\nsubmission.to_csv('test_pred_4.csv', index=False)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2022-08-31T22:57:06.104524Z","iopub.execute_input":"2022-08-31T22:57:06.105437Z","iopub.status.idle":"2022-08-31T22:57:10.02354Z","shell.execute_reply.started":"2022-08-31T22:57:06.105396Z","shell.execute_reply":"2022-08-31T22:57:10.022451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"14\"></a> <br>\n# Tensorflow ANN Model","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nprint(\"Tensorflow version:\",tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T22:57:10.025143Z","iopub.execute_input":"2022-08-31T22:57:10.02643Z","iopub.status.idle":"2022-08-31T22:57:11.76711Z","shell.execute_reply.started":"2022-08-31T22:57:10.026388Z","shell.execute_reply":"2022-08-31T22:57:11.765878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n\ndef build_model():\n    model = keras.Sequential()\n    model.add(Dense(256, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.2))\n    model.add(Dense(128, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.2))\n    model.add(Dense(64, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.2))\n    model.add(Dense(64, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.2))\n    model.add(Dense(1, activation='sigmoid'))\n    \n    model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=['accuracy'])\n   \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-08-31T22:57:11.768843Z","iopub.execute_input":"2022-08-31T22:57:11.769704Z","iopub.status.idle":"2022-08-31T22:57:11.778966Z","shell.execute_reply.started":"2022-08-31T22:57:11.769664Z","shell.execute_reply":"2022-08-31T22:57:11.777813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def step_decay(epoch):\n    decay_rate = 1\n    initial_learning_rate = 0.01\n    lrate = initial_learning_rate*(1/(1+(decay_rate*epoch)))\n#     print(\"Learning Rate: \",lrate)\n#     print(\"Epoch: \",epoch)\n    return lrate\n\n\nlrate = tf.keras.callbacks.LearningRateScheduler(step_decay)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T22:57:11.780464Z","iopub.execute_input":"2022-08-31T22:57:11.781511Z","iopub.status.idle":"2022-08-31T22:57:11.788419Z","shell.execute_reply.started":"2022-08-31T22:57:11.781472Z","shell.execute_reply":"2022-08-31T22:57:11.787478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_history(history):\n    \n    loss = history.history[\"loss\"]\n    val_loss = history.history[\"val_loss\"]\n    epochs = range(1, len(loss) + 1)\n    plot1 = plt.figure(1)\n    plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n    plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n    plt.title(\"Training and validation loss\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show\n    \n    acc = history.history[\"accuracy\"]\n    val_acc = history.history[\"val_accuracy\"]\n    plot2 = plt.figure(2)\n    plt.plot(epochs, acc, \"bo\", label=\"Training accuracy\")\n    plt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\n    plt.title(\"Training and validation accuracy\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-31T22:57:11.790188Z","iopub.execute_input":"2022-08-31T22:57:11.790556Z","iopub.status.idle":"2022-08-31T22:57:11.80224Z","shell.execute_reply.started":"2022-08-31T22:57:11.79052Z","shell.execute_reply":"2022-08-31T22:57:11.801273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []  # roc auc scores\n\nfor fold in range(5): #5\n    x_train =  df_train_full[df_train_full.kfold != fold].reset_index(drop=True)\n    x_val = df_train_full[df_train_full.kfold == fold].reset_index(drop=True)\n    x_test = df_test.copy()\n    \n    valid_ids = x_val.id.values.tolist()\n#     print(valid_ids)\n    \n    y_train = x_train.failure\n    y_val = x_val.failure\n    \n    # to drop target colum\n    x_train = x_train.drop(['id','failure'], axis=1)\n    x_val = x_val.drop(['id','failure'], axis=1)\n#     print(x_train.head())\n    \n    #preprocess\n    preprocessor = ColumnTransformer([\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n        ])\n    \n    x_train = do_preprocess(x_train, preprocessor, train=True, missing_columns=True)\n    x_val = do_preprocess(x_val, preprocessor, train=False, missing_columns=True)\n    x_test = do_preprocess(x_test, preprocessor, train=False, missing_columns=True)\n    \n    # select model\n    model = build_model()\n    num_epochs = 200\n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=20)\n    history = model.fit(x_train, y_train, \n                        validation_data=(x_val, y_val), \n                        epochs=num_epochs, \n                        batch_size=2048, \n                        verbose=0, \n                        callbacks=[lrate, early_stopping])\n\n    plot_history(history)\n    \n    #Evaluating on Validation Set\n    y_val_pred = model.predict(x_val).reshape(-1)\n    score = roc_auc_score(y_val, y_val_pred)\n    print(f\"auc = {score:.5f}\")\n    scores.append(score)\n    \n    # predict test set\n    y_test_pred = model.predict(x_test).reshape(-1)\n        \n    # save test and validation predictions on a list and a dict\n    final_test_predictions.append(y_test_pred)\n    final_valid_predictions.update(dict(zip(valid_ids, y_val_pred)))\n    \n\nprint(np.mean(scores), np.std(scores))\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_5\"]\nfinal_valid_predictions.to_csv(\"train_pred_5.csv\", index=False)\n\nsubmission = pd.DataFrame({'id': df_test.index, 'pred_5': y_test_pred})\nsubmission['pred_5'] = np.mean(np.column_stack(final_test_predictions), axis=1)\nsubmission.to_csv('test_pred_5.csv', index=False)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2022-08-31T22:57:11.804246Z","iopub.execute_input":"2022-08-31T22:57:11.804653Z","iopub.status.idle":"2022-08-31T22:58:36.546101Z","shell.execute_reply.started":"2022-08-31T22:57:11.804617Z","shell.execute_reply":"2022-08-31T22:58:36.544709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"17\"></a> <br>\n# Merge New Sets","metadata":{}},{"cell_type":"code","source":"df1 = pd.read_csv(\"train_pred_1.csv\")\ndf2 = pd.read_csv(\"train_pred_2.csv\")\ndf3 = pd.read_csv(\"train_pred_3.csv\")\ndf4 = pd.read_csv(\"train_pred_4.csv\")\ndf5 = pd.read_csv(\"train_pred_5.csv\")\n\ndf_test1 = pd.read_csv(\"test_pred_1.csv\")\ndf_test2 = pd.read_csv(\"test_pred_2.csv\")\ndf_test3 = pd.read_csv(\"test_pred_3.csv\")\ndf_test4 = pd.read_csv(\"test_pred_4.csv\")\ndf_test5 = pd.read_csv(\"test_pred_5.csv\")\n\ndf = df_train_full.copy()\n\ndf = df.merge(df1, on=\"id\", how=\"left\")\ndf = df.merge(df2, on=\"id\", how=\"left\")\ndf = df.merge(df3, on=\"id\", how=\"left\")\ndf = df.merge(df4, on=\"id\", how=\"left\")\ndf = df.merge(df5, on=\"id\", how=\"left\")\n\ndf_test = df_test.merge(df_test1, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test2, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test3, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test4, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test5, on=\"id\", how=\"left\")\n\nprint(df.columns)\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-31T22:58:36.548449Z","iopub.execute_input":"2022-08-31T22:58:36.549181Z","iopub.status.idle":"2022-08-31T22:58:36.75354Z","shell.execute_reply.started":"2022-08-31T22:58:36.54912Z","shell.execute_reply":"2022-08-31T22:58:36.752422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-31T22:58:36.754969Z","iopub.execute_input":"2022-08-31T22:58:36.755852Z","iopub.status.idle":"2022-08-31T22:58:36.780099Z","shell.execute_reply.started":"2022-08-31T22:58:36.755813Z","shell.execute_reply":"2022-08-31T22:58:36.779023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2022-08-31T22:58:36.781657Z","iopub.execute_input":"2022-08-31T22:58:36.782111Z","iopub.status.idle":"2022-08-31T22:58:36.789904Z","shell.execute_reply.started":"2022-08-31T22:58:36.782074Z","shell.execute_reply":"2022-08-31T22:58:36.788171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"18\"></a> <br>\n# Blending Results","metadata":{}},{"cell_type":"code","source":"useful_features = [\"pred_1\", \"pred_2\", \"pred_3\", \"pred_4\", \"pred_5\"]\n\nnumerical_cols_2 = [c for c in df_train.columns if df_train[c].dtypes in ['int', 'float']]\nprint(\"Numerical Columns\\n\", numerical_cols)\n\ncategorical_cols = [c for c in df_train.columns if df_train[c].dtypes in ['object']]\nprint(\"\\nCategorical Columns\\n\", categorical_cols)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T22:58:36.792458Z","iopub.execute_input":"2022-08-31T22:58:36.792813Z","iopub.status.idle":"2022-08-31T22:58:36.801551Z","shell.execute_reply.started":"2022-08-31T22:58:36.792778Z","shell.execute_reply":"2022-08-31T22:58:36.800374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"useful_features = [\"pred_1\", \"pred_2\", \"pred_3\", \"pred_4\", \"pred_5\"]\n# df_test = df_test[useful_features]\n\n# useful_features = np.concatenate((numerical_cols_2, categorical_cols, useful_features))\n\nfinal_predictions = []\nscores = []\n\nfor fold in range(5):\n    x_train =  df[df.kfold != fold]\n    x_val = df[df.kfold == fold]\n    x_test = df_test.copy()\n\n    y_train = x_train.failure\n    y_val = x_val.failure\n\n        \n    x_train = x_train[useful_features]\n    x_val = x_val[useful_features]\n    x_test = x_test[useful_features]\n    \n    #preprocess\n    preprocessor = StandardScaler()\n    \n    x_train = preprocessor.fit_transform(x_train)\n    x_val = preprocessor.transform(x_val)\n    x_test =  preprocessor.transform(x_test)\n    \n    # train model\n#     model = LogisticRegression()\n    model = LogisticRegression(max_iter=500, C=0.0001, penalty='l2', solver='newton-cg')\n    model.fit(x_train, y_train)\n    \n    #Evaluating on Validation Set\n    y_val_pred = model.predict_proba(x_val)[:,1]\n    score = roc_auc_score(y_val, y_val_pred)\n    print(f\"auc = {score:.5f}\")\n    scores.append(score)\n    \n    # predict test set\n    y_test_pred = model.predict_proba(x_test)[:,1]  \n    \n    preds_valid = model.predict(x_val)\n    test_preds = model.predict(x_test)\n    \n    final_predictions.append(y_test_pred)\n    \n\nprint(np.mean(scores), np.std(scores))","metadata":{"execution":{"iopub.status.busy":"2022-08-31T22:58:36.80362Z","iopub.execute_input":"2022-08-31T22:58:36.804526Z","iopub.status.idle":"2022-08-31T22:58:37.477047Z","shell.execute_reply.started":"2022-08-31T22:58:36.804384Z","shell.execute_reply":"2022-08-31T22:58:37.476009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_importances = model.coef_.ravel()\nmodel_names = [\"Logistic Regression\", \"XGBoost\", \"CatBoostClassifier\", \"LGBMClassifier\", 'ANN']\nimportance_df = plot_model_feature_importance(feature_importances, model_names, number_of_features=20)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T22:58:37.478966Z","iopub.execute_input":"2022-08-31T22:58:37.479697Z","iopub.status.idle":"2022-08-31T22:58:37.70034Z","shell.execute_reply.started":"2022-08-31T22:58:37.479658Z","shell.execute_reply":"2022-08-31T22:58:37.699405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"30\"></a> <br>\n# Submission","metadata":{}},{"cell_type":"code","source":"sample_submission = pd.read_csv('/kaggle/input/tabular-playground-series-aug-2022/sample_submission.csv')\nsample_submission.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-31T22:58:37.701959Z","iopub.execute_input":"2022-08-31T22:58:37.702354Z","iopub.status.idle":"2022-08-31T22:58:37.720275Z","shell.execute_reply.started":"2022-08-31T22:58:37.702308Z","shell.execute_reply":"2022-08-31T22:58:37.719167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.failure = np.mean(np.column_stack(final_predictions), axis=1)\nsample_submission.to_csv(\"submission.csv\", index=False)\nsample_submission","metadata":{"execution":{"iopub.status.busy":"2022-08-31T22:58:37.72171Z","iopub.execute_input":"2022-08-31T22:58:37.722198Z","iopub.status.idle":"2022-08-31T22:58:37.780695Z","shell.execute_reply.started":"2022-08-31T22:58:37.72216Z","shell.execute_reply":"2022-08-31T22:58:37.779413Z"},"trusted":true},"execution_count":null,"outputs":[]}]}